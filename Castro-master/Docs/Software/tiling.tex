Overview of the tiling approach in C++ BoxLib

logical tiling: data storage is the same (box is continuous in
memory), but how we loop over it is different

separate tiling: data storage changes to reflect how the tiling is done



old way:

for (MFIter mfi(mf); mfi.isValid(); ++mfi)
{

  const Box& bx = mfi.validbox();  // doesn't include ghost cells
  BL_FORT_FAB(mf[mfi])   // data pointer + index space (including ghost cells)
              bx.loVect, hiVect  // valid region to loop over
  
}


if we have 3 boxes, then this loop will have 3 iterations



in tiling approach, we change to

mfi(mf, true)   // turn on tiliing
mfi(mf, iv)     // iv is an InteVect (iv(1024,8,8) -- explicitly give tile size)
// can set tile size in inputs file  mfiter_tile_size

validBox does not change, so we should not use it, but instead use .tilebox(); -- this
gives the lo() and hi() for the tile.


if we have box 0 with 4 tiles, box 1 with 4 tiles, and box 2 with 6
tiles, then the for loop will have 14 iterations.  With pure MPI, we'd
loop over these separately.

There may be pure MPI advantages -- tiles are smaller than boxes,
since we allocate smaller 


Tiling can naturally transition from all threads working on a single box to
each thread working on a single box as the boxes coarsen (e.g. in multigrid)


issues:

-- when we allocate temporaries, only allocate on the tile box

-- lo and hi are now the region that we work on, not the valid box
   
-- if you pass in an mf, if you are doing Gauss-Siedel, then you are
   not safe

-- there were some places where we were using xlo to refer to boxes
   corner, but now it is always corresponds to lo() (i.e. if it is a
   tile, that is fine


OMP:

The OpenMP is now all in C++ -- no longer in Fortran

# pragma omp parallel    // no 'for' with an iterator


need to make sure that it is safe to do OpenMP

tileboxes are non-overlapping
union of tileboxes cover the fab

consider node-centered ugdnv and cell-centered s

if you do mfi(s) -- your tiling is based on cell-center

if s is 8x8, and you are doing 4 tiles, so you will have 0 -> 3; 4 -> 7

if you do mfi(ugdnv) -- your tiling is based on nodal indices, so yow
will have 0 -> 3; 4 -> 8

hydro is a place where we don't tile in C++, but instead do it in the
Fortran.  The reason is that we'd need ghost cells for the temporary
tile-sized arrays in computing all the intermediate stuff in doing
the hydro

if you find yourself doing a lot of temporary tile-sized arrays
that require ghostcells, then doing to tiling in C++ is bad
(extra memory + redundant calculations)



when updating a problem to work with the tiling

-- look at Fortran do loops -- if we allocate

   a(a_l1:a_h1, a_l2:a_h2, ...)

   we cannot have loops where we do:

   do a_l1, a_h1

   but instead we need to do lo, hi

-- we can no longer do a = 0, but instead need to
   do a(lo:hi) = 0



when not to use this tiling?
   
dynamic scheduling is better for burning loading balancing than tiling


replacements for tilebox()

growntilebox(ng) -- this will divide the valid region + its ghost cells (at the box-level)
into files

nodalbox({0,1,2}) -- suppose the mf is cell-centered, then the nodalbox will
                     will return tiles lo, hi that are tiled in the specified
                     direction

                  
 



   





   


